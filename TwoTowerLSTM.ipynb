{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF53z40ZBIeJgFN6dRgEBz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmender2/DataScience/blob/main/TwoTowerLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/au_train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/au_test.csv')\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "# select numerical and categorical features\n",
        "num_cols = train.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "if 'Id' in num_cols:\n",
        "    num_cols.remove('Id')\n",
        "print(f\"Total numerical features: {len(num_cols)}\")\n",
        "print(f\"Total categorical features: {len(cat_cols)}\")\n",
        "print(num_cols)\n",
        "print(cat_cols)"
      ],
      "metadata": {
        "id": "spYmUIgKR7uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_scaler = preprocessing.StandardScaler()\n",
        "mm_scaler = preprocessing.MinMaxScaler()\n",
        "robust_scaler = preprocessing.RobustScaler()\n",
        "quantile_scaler = preprocessing.QuantileTransformer(n_quantiles=60, output_distribution='normal')"
      ],
      "metadata": {
        "id": "YyPTTxxuSHBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_train_df = train.copy(deep=True)\n",
        "scaled_test_df = test.copy(deep=True)\n",
        "scaled_train_df[num_cols] = mm_scaler.fit_transform(scaled_train_df[num_cols])\n",
        "scaled_test_df[num_cols] = mm_scaler.fit_transform(scaled_test_df[num_cols])"
      ],
      "metadata": {
        "id": "5I1i5UzXSHnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAcrLcPovw3d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X = scaled_train_df.drop('class', axis = 1)\n",
        "y = scaled_train_df[['class']]\n",
        "XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.3)\n",
        "timeSteps = 10\n",
        "# Encode categorical columns in X_train\n",
        "catTrain = []\n",
        "catTest = []\n",
        "# Define the encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')  # 'ignore' helps to handle categorical levels present in test but not in train\n",
        "\n",
        "# Fit the encoder on training data\n",
        "if ('class' in cat_cols):\n",
        "  cat_cols.remove('class')\n",
        "encoder.fit(XTrain[cat_cols])\n",
        "\n",
        "# Transform both training and test datasets\n",
        "catTrain = encoder.transform(XTrain[cat_cols])\n",
        "catTest = encoder.transform(XTest[cat_cols])\n",
        "\n",
        "catTrain_df = pd.DataFrame(catTrain.toarray(), columns=encoder.get_feature_names_out(input_features=cat_cols))\n",
        "catTest_df = pd.DataFrame(catTest.toarray(), columns=encoder.get_feature_names_out(input_features=cat_cols))\n",
        "num_cols = XTrain.select_dtypes(include=[np.number]).columns.tolist()\n",
        "# Select numerical columns\n",
        "numTrain = XTrain[num_cols]\n",
        "numTest = XTest[num_cols]\n",
        "\n",
        "numTrainR = np.array([numTrain[i:i + timeSteps] for i in range(len(numTrain) - timeSteps + 1)])\n",
        "numTestR = np.array([numTest[i:i + timeSteps] for i in range(len(numTest) - timeSteps + 1)])\n",
        "catTrainR = np.array([catTrain_df[i:i + timeSteps] for i in range(len(catTrain_df) - timeSteps + 1)])\n",
        "catTestR = np.array([catTest_df[i:i + timeSteps] for i in range(len(catTest_df) - timeSteps + 1)])\n",
        "yTrainB = np.where(yTrain == ' >50K', 1, 0)\n",
        "yTestB = np.where(yTest == ' >50K', 1, 0)\n",
        "yTrainReshaped = yTrainB[timeSteps - 1 :]\n",
        "yTestReshaped = yTestB[timeSteps - 1 :]\n",
        "\n",
        "layer_sizesC=[timeSteps, catTrainR.shape[2]]\n",
        "layer_sizesN=[timeSteps, numTrainR.shape[2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuPYxXu1B6qg"
      },
      "outputs": [],
      "source": [
        "class HouseFeatureTower(tf.keras.layers.Layer):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super().__init__()\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(64, input_shape=(layer_sizes[0], layer_sizes[1]), return_sequences=True),\n",
        "            tf.keras.layers.Dense(units=32),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        print(f'HouseFeatureTower input shape: {inputs.shape}')  # Add this line\n",
        "        return self.encoder(inputs)\n",
        "        print(f'output shape in HouseFeatureTower call: {output.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_bjBVHGCck7"
      },
      "outputs": [],
      "source": [
        "class SalePriceTower(tf.keras.layers.Layer):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super().__init__()\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(64, input_shape=(layer_sizes[0], layer_sizes[1]), return_sequences=True),\n",
        "            tf.keras.layers.Dense(units=32),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "    def call(self, inputs):\n",
        "        print(f'Q input shape: {inputs.shape}')  # Add this line\n",
        "        return self.encoder(inputs)\n",
        "        print(f'output shape in Q call: {output.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMF_EWHiCjn9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class HousePricePredictionModel(tfrs.models.Model):\n",
        "    def __init__(self, layer_sizesN, layer_sizesC):\n",
        "        super(HousePricePredictionModel, self).__init__()\n",
        "        self.query_tower = HouseFeatureTower(layer_sizesN)\n",
        "        self.candidate_tower = SalePriceTower(layer_sizesC)\n",
        "        self.lstm1N = layers.LSTM(64, return_sequences=False)  # LSTM for the output of query_tower\n",
        "        self.lstm1C = layers.LSTM(32, return_sequences=False)  # LSTM for the output of candidate_tower\n",
        "        self.concatenate = layers.Concatenate(axis=-1)\n",
        "        self.dense = layers.Dense(1, activation='sigmoid')  # Adjust the number of units and activation function as necessary\n",
        "        self.final_dense = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        # Use binary cross-entropy as the loss function\n",
        "        y_pred = tf.keras.activations.sigmoid(y_pred)\n",
        "        loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            y = tf.reshape(y, [-1, 1])\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y_true = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_pred = self(x, training=False)\n",
        "\n",
        "        # Updates the metrics tracking the loss\n",
        "        self.compiled_loss(y_true, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y_true, y_pred)\n",
        "\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "    def call(self, inputs, training=False):\n",
        "        q = inputs[0]\n",
        "        c = inputs[1]\n",
        "        query_embeddings = self.query_tower(q)\n",
        "        candidate_embeddings = self.candidate_tower(c)\n",
        "        print(\"Query embeddings shape:\", query_embeddings.shape)\n",
        "        print(\"Candidate embeddings shape:\", candidate_embeddings.shape)\n",
        "        # Expand dimensions to make the output 3D for LSTM layer\n",
        "        #query_embeddings = tf.expand_dims(query_embeddings, 1)\n",
        "        #candidate_embeddings = tf.expand_dims(candidate_embeddings, 1)\n",
        "\n",
        "        lstm_outN = self.lstm1N(query_embeddings)\n",
        "        lstm_outC = self.lstm1C(candidate_embeddings)\n",
        "        combined = self.concatenate([lstm_outN, lstm_outC])\n",
        "        x = self.dense(combined)\n",
        "        output = self.final_dense(x)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = HousePricePredictionModel(layer_sizesN, layer_sizesC)"
      ],
      "metadata": {
        "id": "eRN_V1bZTzuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "HHdTPyQNT0KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(catTrain_df)"
      ],
      "metadata": {
        "id": "-vwrZsdL4dbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([numTrainR, catTrainR], yTrainReshaped, epochs=10, verbose=1, validation_data=([numTestR,catTestR], yTestReshaped))\n"
      ],
      "metadata": {
        "id": "R1qbmUgJT1jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical columns in X_train\n",
        "numTest = []\n",
        "catTest = []\n",
        "# Define the encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')  # 'ignore' helps to handle categorical levels present in test but not in train\n",
        "\n",
        "# Fit the encoder on training data\n",
        "if ('class' in cat_cols):\n",
        "  cat_cols.remove('class')\n",
        "encoder.fit(scaled_test_df[cat_cols])\n",
        "# Transform both training and test datasets\n",
        "catTest = encoder.transform(scaled_test_df[cat_cols])\n",
        "catTest_df = pd.DataFrame(catTest.toarray(), columns=encoder.get_feature_names_out(input_features=cat_cols))\n",
        "\n",
        "\n",
        "missing_col = set(catTrain_df.columns) - set(catTest_df.columns)\n",
        "for col in missing_col:\n",
        "    catTest_df[col] = 0\n",
        "\n",
        "num_cols = scaled_test_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "# Select numerical columns\n",
        "numTest = scaled_test_df[num_cols]\n",
        "\n",
        "numTestR = np.array([numTest[i:i + timeSteps] for i in range(len(numTest) - timeSteps + 1)])\n",
        "catTestR = np.array([catTest_df[i:i + timeSteps] for i in range(len(catTest_df) - timeSteps + 1)])\n",
        "\n",
        "yTestB = np.where(scaled_test_df['class'] == ' >50K', 1, 0)\n"
      ],
      "metadata": {
        "id": "tLTxw3j4I_lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model((numTestR, catTestR))"
      ],
      "metadata": {
        "id": "UzpvKTIvmAwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate thresholds\n",
        "num_thresholds = 100\n",
        "thresholds = np.linspace(0, 1, num_thresholds)\n",
        "\n",
        "# Initialize variables for optimal threshold and corresponding metric\n",
        "best_f1 = 0\n",
        "optimal_threshold = 0\n",
        "\n",
        "# Iterate over thresholds and compute evaluation metrics\n",
        "j = 0\n",
        "for threshold in thresholds:\n",
        "    # Convert probabilities into class labels based on threshold\n",
        "    predicted_labels = np.where(np.array(predictions) >= threshold, 1, 0)\n",
        "\n",
        "    # Calculate evaluation metric (e.g., F1-score) using the true labels (y_test) and predicted labels\n",
        "    f1 = f1_score(yTestB, predicted_labels)\n",
        "    j += 1\n",
        "    # Update optimal threshold and metric if a higher F1-score is achieved\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        optimal_threshold = threshold\n",
        "\n",
        "print(\"Optimal Threshold:\", optimal_threshold)\n",
        "print(\"Best F1-score:\", best_f1)\n"
      ],
      "metadata": {
        "id": "hWP3-LKAqlfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(yTestB, np.where(predictions > optimal_threshold, 1, 0))\n",
        "recall = recall_score(yTestB, np.where(predictions > optimal_threshold, 1, 0))\n",
        "f1 = f1_score(yTestB, np.where(predictions > optimal_threshold, 1, 0))\n",
        "auroc = roc_auc_score(yTestB, np.where(predictions > optimal_threshold, 1, 0))\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 Score:', f1)\n",
        "print('AUROC Score:', auroc)"
      ],
      "metadata": {
        "id": "Fon4av2uYb6H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}